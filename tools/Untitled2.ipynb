{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a919a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clue = '/home/t-yaqingwang/Projects/Few-shot-Learning/data/clue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af948080",
   "metadata": {},
   "outputs": [],
   "source": [
    "ust = '/home/t-yaqingwang/Projects/Few-shot-Learning/data/ust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = mpqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0014aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/t-yaqingwang/Projects/Few-shot-Learning/data/clue/mpqa/10-1/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ed27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset utils for different data settings for GLUE.\"\"\"\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from filelock import FileLock\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import transformers\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers import DataProcessor, InputExample\n",
    "from transformers.data.processors.glue import *\n",
    "from transformers.data.metrics import glue_compute_metrics\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Union\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"un_train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = line[0]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class MnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"premise\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_large_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train_large.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train_large.tsv\")), \"train_large\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"un_train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")), \"dev_matched\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test_matched\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "    def get_mappings(self):\n",
    "        return {'contradiction': 'No', 'entailment': 'Yes', 'neutral': 'Maybe'}\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[8].lower()\n",
    "            text_b = line[9].lower()\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class MnliMismatchedProcessor(MnliProcessor):\n",
    "    \"\"\"Processor for the MultiNLI Mismatched data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev_mismatched.tsv\")), \"dev_mismatched\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_mismatched.tsv\")), \"test_mismatched\")\n",
    "\n",
    "\n",
    "class CLUE_MnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"premise\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_clue_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_large_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train_large.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train_large.tsv\")), \"train_large\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"un_train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "      \n",
    "        return self._create_clue_examples(self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")), \"dev_matched\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test_matched\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "    def get_mappings(self):\n",
    "        return {'contradiction': 'No', 'entailment': 'Yes', 'neutral': 'Maybe'}\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "\n",
    "            text_a = line[8].lower()\n",
    "            text_b = line[9].lower()\n",
    "          \n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _create_clue_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[-3].lower()\n",
    "            text_b = line[-2].lower()\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "\n",
    "class CLUE_MnliMismatchedProcessor(CLUE_MnliProcessor):\n",
    "    \"\"\"Processor for the MultiNLI Mismatched data set (GLUE version).\"\"\"\n",
    "\n",
    "    def _create_clue_test_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[1].lower()\n",
    "            text_b = line[2].lower()\n",
    "            label = line[4]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_mismatched.tsv\")), \"test_mismatched\")\n",
    "\n",
    "\n",
    "\n",
    "class CLUE_MnliClueProcessor(CLUE_MnliProcessor):\n",
    "    \"\"\"Processor for the MultiNLI Mismatched data set (GLUE version).\"\"\"\n",
    "\n",
    "    def _create_clue_test_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[1].lower()\n",
    "            text_b = line[2].lower()\n",
    "            label = line[4]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_clue_test_examples(self._read_tsv(os.path.join(data_dir, \"test_clue.tsv\")), \"test_mismatched\")\n",
    "\n",
    "\n",
    "class SnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"premise\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[7]\n",
    "            text_b = line[8]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class ColaProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n",
    "            None,\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        test_mode = set_type == \"test\"\n",
    "        text_index = 3\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[text_index]\n",
    "            label = line[1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class Sst2Processor(DataProcessor):\n",
    "    \"\"\"Processor for the SST-2 data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n",
    "            None,\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        text_index = 0\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[text_index]\n",
    "            label = line[1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "\n",
    "class CLUE_Sst2Processor(DataProcessor):\n",
    "    \"\"\"Processor for the SST-2 data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n",
    "            None,\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_CLUE_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_CLUE_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        text_index = 0\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[text_index]\n",
    "            label = line[1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "    def _create_CLUE_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[-2]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class CLUE_Sst2ClueProcessor(CLUE_Sst2Processor):\n",
    "    \"\"\"Processor for the MultiNLI Mismatched data set (GLUE version).\"\"\"\n",
    "\n",
    "    def _create_CLUE_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[-2]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_CLUE_examples(self._read_tsv(os.path.join(data_dir, \"test_clue.tsv\")), \"test_clue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class StsbProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the STS-B data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [None]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[7]\n",
    "            text_b = line[8]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class QqpProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the QQP data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"question1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"question2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        test_mode = set_type == \"test\"\n",
    "        q1_index = 3\n",
    "        q2_index = 4\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            try:\n",
    "                text_a = line[q1_index]\n",
    "                text_b = line[q2_index]\n",
    "                label = line[5]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class CLUE_QqpProcessor(QqpProcessor):\n",
    "    \"\"\"Processor for the QQP data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"question1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"question2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_clue.tsv\")), \"test-clue\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        test_mode = set_type == \"test\"\n",
    "        q1_index = 3\n",
    "        q2_index = 4\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            try:\n",
    "                text_a = line[q1_index]\n",
    "                text_b = line[q2_index]\n",
    "                label = line[5]\n",
    "            except IndexError:\n",
    "                continue\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class QnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the QNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"question\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[1]\n",
    "            text_b = line[2]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class RteProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the RTE data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[1]\n",
    "            text_b = line[2]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class CLUE_RteProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the RTE data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[1]\n",
    "            text_b = line[2]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "\n",
    "class CLUE_RteClueProcessor(CLUE_RteProcessor):\n",
    "    \"\"\"Processor for the MultiNLI Mismatched data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test_clue.tsv\")), \"test_clue\")\n",
    "\n",
    "\n",
    "class WnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the WNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"un_train.tsv\")), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text_a = line[1]\n",
    "            text_b = line[2]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "class TextClassificationProcessor(DataProcessor):\n",
    "    \"\"\"\n",
    "    Data processor for text classification datasets (mr, sst-5, subj, trec, cr, mpqa).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task_name):\n",
    "        self.task_name = task_name \n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n",
    "            None,\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "  \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(pd.read_csv(os.path.join(data_dir, \"train.csv\"), header=None).values.tolist(), \"train\")\n",
    "\n",
    "    def get_un_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(pd.read_csv(os.path.join(data_dir, \"un_train.csv\"), header=None).values.tolist(), \"un_train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(pd.read_csv(os.path.join(data_dir, \"dev.csv\"), header=None).values.tolist(), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(pd.read_csv(os.path.join(data_dir, \"test.csv\"), header=None).values.tolist(), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if self.task_name == \"mr\":\n",
    "            return list(range(2))\n",
    "        elif self.task_name == \"sst-5\":\n",
    "            return list(range(5))\n",
    "        elif self.task_name == \"subj\":\n",
    "            return list(range(2))\n",
    "        elif self.task_name == \"trec\":\n",
    "            return list(range(6))\n",
    "        elif self.task_name == \"cr\":\n",
    "            return list(range(2))\n",
    "        elif self.task_name == \"mpqa\":\n",
    "            return list(range(2))\n",
    "        else:\n",
    "            raise Exception(\"task_name not supported.\")\n",
    "        \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            if self.task_name == \"ag_news\":\n",
    "                examples.append(InputExample(guid=guid, text_a=line[1] + '. ' + line[2], short_text=line[1] + \".\", label=line[0]))\n",
    "            elif self.task_name == \"yelp_review_full\":\n",
    "                examples.append(InputExample(guid=guid, text_a=line[1], short_text=line[1], label=line[0]))\n",
    "            elif self.task_name == \"yahoo_answers\":\n",
    "                text = line[1]\n",
    "                if not pd.isna(line[2]):\n",
    "                    text += ' ' + line[2]\n",
    "                if not pd.isna(line[3]):\n",
    "                    text += ' ' + line[3]\n",
    "                examples.append(InputExample(guid=guid, text_a=text, short_text=line[1], label=line[0])) \n",
    "            elif self.task_name in ['mr', 'sst-5', 'subj', 'trec', 'cr', 'mpqa']:\n",
    "                examples.append(InputExample(guid=guid, text_a=line[1], label=line[0]))\n",
    "            else:\n",
    "                raise Exception(\"Task_name not supported.\")\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "class CLUE_TextClassificationProcessor(TextClassificationProcessor):\n",
    "    \"\"\"\n",
    "    Data processor for text classification datasets (mr, sst-5, subj, trec, cr, mpqa).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(pd.read_csv(os.path.join(data_dir, \"test_clue.csv\"), header=None).values.tolist(),\n",
    "                                     \"test-clue\")\n",
    "\n",
    "\n",
    "def text_classification_metrics(task_name, preds, labels):\n",
    "    return {\"acc\": (preds == labels).mean()}\n",
    "\n",
    "# Add your task to the following mappings\n",
    "\n",
    "processors_mapping = {\n",
    "    \"cola\": ColaProcessor(),\n",
    "    \"mnli\": MnliProcessor(),\n",
    "    \"mnli-mm\": MnliMismatchedProcessor(),\n",
    "    \"mrpc\": MrpcProcessor(),\n",
    "    \"sst-2\": Sst2Processor(),\n",
    "    \"sts-b\": StsbProcessor(),\n",
    "    \"qqp\": QqpProcessor(),\n",
    "    \"qnli\": QnliProcessor(),\n",
    "    \"rte\": RteProcessor(),\n",
    "    \"wnli\": WnliProcessor(),\n",
    "    \"snli\": SnliProcessor(),\n",
    "    \"mr\": TextClassificationProcessor(\"mr\"),\n",
    "    \"sst-5\": TextClassificationProcessor(\"sst-5\"),\n",
    "    \"subj\": TextClassificationProcessor(\"subj\"),\n",
    "    \"trec\": TextClassificationProcessor(\"trec\"),\n",
    "    \"cr\": TextClassificationProcessor(\"cr\"),\n",
    "    \"mpqa\": TextClassificationProcessor(\"mpqa\")\n",
    "}\n",
    "\n",
    "CLUE_processors_mapping = {\n",
    "    \"cola\": ColaProcessor(),\n",
    "    \"mnli\": CLUE_MnliProcessor(),\n",
    "    \"mnli-clue\": CLUE_MnliClueProcessor(),\n",
    "    \"mnli-mm\": CLUE_MnliMismatchedProcessor(),\n",
    "    \"mrpc\": MrpcProcessor(),\n",
    "    \"sst-2\": CLUE_Sst2Processor(),\n",
    "    \"sst-2-clue\": CLUE_Sst2ClueProcessor(),\n",
    "    \"sts-b\": StsbProcessor(),\n",
    "    \"qqp\": QqpProcessor(),\n",
    "    \"qqp-clue\": CLUE_QqpProcessor(),\n",
    "    \"qnli\": QnliProcessor(),\n",
    "    \"rte\": CLUE_RteProcessor(),\n",
    "    \"rte-clue\": CLUE_RteClueProcessor(),\n",
    "    \"wnli\": WnliProcessor(),\n",
    "    \"snli\": SnliProcessor(),\n",
    "    \"mr\": TextClassificationProcessor(\"mr\"),\n",
    "    \"mr-clue\":  CLUE_TextClassificationProcessor(\"mr\"),\n",
    "    \"sst-5\": TextClassificationProcessor(\"sst-5\"),\n",
    "    \"subj\": TextClassificationProcessor(\"subj\"),\n",
    "    \"subj-clue\": CLUE_TextClassificationProcessor(\"subj\"),\n",
    "    \"trec\": TextClassificationProcessor(\"trec\"),\n",
    "    \"cr\": TextClassificationProcessor(\"cr\"),\n",
    "    \"mpqa\": TextClassificationProcessor(\"mpqa\"),\n",
    "   \"mpqa-clue\": CLUE_TextClassificationProcessor(\"mpqa\")\n",
    "}\n",
    "\n",
    "num_labels_mapping = {\n",
    "    \"cola\": 2,\n",
    "    \"mnli\": 3,\n",
    "    \"mrpc\": 2,\n",
    "    \"sst-2\": 2,\n",
    "    \"sts-b\": 1,\n",
    "    \"qqp\": 2,\n",
    "    \"qnli\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"wnli\": 2,\n",
    "    \"snli\": 3,\n",
    "    \"mr\": 2,\n",
    "    \"sst-5\": 5,\n",
    "    \"subj\": 2,\n",
    "    \"trec\": 6,\n",
    "    \"cr\": 2,\n",
    "    \"mpqa\": 2\n",
    "}\n",
    "\n",
    "output_modes_mapping = {\n",
    "    \"cola\": \"classification\",\n",
    "    \"mnli\": \"classification\",\n",
    "    \"mnli-mm\": \"classification\",\n",
    "    \"mnli-mm-clue\": \"classification\",\n",
    "    \"mrpc\": \"classification\",\n",
    "    \"sst-2\": \"classification\",\n",
    "    \"sst-2-clue\": \"classification\",\n",
    "    \"sts-b\": \"regression\",\n",
    "    \"qqp\": \"classification\",\n",
    "    \"qqp-clue\": \"classification\",\n",
    "    \"qnli\": \"classification\",\n",
    "    \"rte\": \"classification\",\n",
    "     \"rte-clue\": \"classification\",\n",
    "    \"wnli\": \"classification\",\n",
    "    \"snli\": \"classification\",\n",
    "    \"mr\": \"classification\",\n",
    "    \"mr-clue\": \"classification\",\n",
    "    \"sst-5\": \"classification\",\n",
    "    \"subj\": \"classification\",\n",
    "    \"subj-clue\": \"classification\",\n",
    "    \"trec\": \"classification\",\n",
    "    \"cr\": \"classification\",\n",
    "    \"mpqa\": \"classification\",\n",
    "    \"mpqa-clue\": \"classification\"\n",
    "}\n",
    "\n",
    "# Return a function that takes (task_name, preds, labels) as inputs\n",
    "compute_metrics_mapping = {\n",
    "    \"cola\": glue_compute_metrics,\n",
    "    \"mnli\": glue_compute_metrics,\n",
    "    \"mnli-clue\": text_classification_metrics,\n",
    "    \"mnli-mm\": glue_compute_metrics,\n",
    "    \"mrpc\": glue_compute_metrics,\n",
    "    \"sst-2\": glue_compute_metrics,\n",
    "    \"sst-2-clue\": text_classification_metrics,\n",
    "    \"sts-b\": glue_compute_metrics,\n",
    "    \"qqp\": glue_compute_metrics,\n",
    "    \"qqp-clue\": text_classification_metrics,\n",
    "    \"qnli\": glue_compute_metrics,\n",
    "    \"rte\": glue_compute_metrics,\n",
    "    \"rte-clue\": text_classification_metrics,\n",
    "    \"wnli\": glue_compute_metrics,\n",
    "    \"snli\": text_classification_metrics,\n",
    "    \"mr\": text_classification_metrics,\n",
    "    \"mr-clue\": text_classification_metrics,\n",
    "    \"sst-5\": text_classification_metrics,\n",
    "    \"subj\": text_classification_metrics,\n",
    "    \"subj-clue\": text_classification_metrics,\n",
    "    \"trec\": text_classification_metrics,\n",
    "    \"cr\": text_classification_metrics,\n",
    "    \"mpqa\": text_classification_metrics,\n",
    "    \"mpqa-clue\": text_classification_metrics,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# For regression task only: median\n",
    "median_mapping = {\n",
    "    \"sts-b\": 2.5\n",
    "}\n",
    "\n",
    "bound_mapping = {\n",
    "    \"sts-b\": (0, 5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b09512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-14-e6378c7facb9>\u001b[0m(20)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     18 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m                    \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 20 \u001b[0;31m                    \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m                        \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m                            \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m87\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> r\n",
      "> \u001b[0;32m<ipython-input-14-e6378c7facb9>\u001b[0m(20)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     18 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m                    \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 20 \u001b[0;31m                    \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m                        \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m                            \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m87\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> len(examples)\n",
      "100\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-14-e6378c7facb9>\u001b[0m(39)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     37 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     38 \u001b[0;31m                \u001b[0;31m#output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 39 \u001b[0;31m                \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     40 \u001b[0;31m                \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e6378c7facb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m#output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-e6378c7facb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m#output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bestfriend36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bestfriend36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for  task_name in [ 'QQP', 'RTE']:\n",
    "    for k in [ 30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            \n",
    "            \n",
    "            output_dir = clue + '/'+task_name+'/'+str(100)+'-'+str(seed)\n",
    "            #os.mkdir(output_dir)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "#             try:\n",
    "#                 os.remove(os.path.join(output_dir, 'test-clue.tsv'))\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['train', 'un_train', 'test', 'test_clue']:\n",
    "                if mode == 'train':\n",
    "                    examples = []\n",
    "                    for k in [30, 35]:\n",
    "                        if k == 35:\n",
    "                            seed = [100, 13, 21, 42, 87][seed-1]\n",
    "                            data_dir = '/home/t-yaqingwang/Projects/Few-shot-Learning/data/k-shot' + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "                        else:\n",
    "                            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "                        examples += processor.get_train_examples(data_dir)\n",
    "                        support_example += examples\n",
    "                        import pdb\n",
    "                        pdb.set_trace()\n",
    "                elif mode == 'un_train':\n",
    "                    examples = processor.get_un_train_examples(data_dir)\n",
    "                elif mode == 'test':\n",
    "                    examples = processor.get_test_examples(data_dir)\n",
    "                elif mode == 'test_clue':\n",
    "                    examples = CLUE_processors_mapping[(task_name+'-clue').lower()].get_test_examples(data_dir)\n",
    "                    \n",
    "                \n",
    "                #output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "                for example in examples:\n",
    "                    text_a = example.text_a\n",
    "                    text_b = example.text_b\n",
    "                    label = example.label\n",
    "#                     output_file.write(text_a+'\\t')\n",
    "#                     if text_b is not None:\n",
    "#                         output_file.write(text_b+'\\t')\n",
    "#                     output_file.write(label+'\\n')\n",
    "#                 output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fac56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for  task_name in [ 'MNLI']:\n",
    "    for k in [10, 20, 30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            output_dir = ust+ '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['train', 'un_train', 'test_matched', 'test_mismatched', 'test_clue']:\n",
    "                if mode == 'train':\n",
    "                    examples = processor.get_train_examples(data_dir)\n",
    "                elif mode == 'un_train':\n",
    "                    examples = processor.get_un_train_examples(data_dir)\n",
    "                elif mode == 'test_machted':\n",
    "                    examples = processor.get_test_examples(data_dir)\n",
    "                elif mode == 'test_mismatched':\n",
    "                    examples = CLUE_processors_mapping['mnli-mm'].get_test_examples(data_dir)\n",
    "                elif mode == 'test_clue':\n",
    "                    examples = CLUE_processors_mapping[(task_name+'-clue').lower()].get_test_examples(data_dir)\n",
    "                    \n",
    "                output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\n",
    "                for example in examples:\n",
    "                    text_a = example.text_a\n",
    "                    text_b = example.text_b\n",
    "                    label = example.label\n",
    "                    output_file.write(text_a+'\\t')\n",
    "                    if text_b is not None:\n",
    "                        output_file.write(text_b+'\\t')\n",
    "                    output_file.write(label+'\\n')\n",
    "                output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "376174b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for  task_name in [ 'subj', 'mpqa']:\n",
    "    for k in [10, 20, 30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            output_dir = ust+ '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                os.remove(os.path.join(output_dir, 'test-clue.csv'))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['train', 'un_train', 'test', 'test_clue']:\n",
    "                if mode == 'train':\n",
    "                    examples = processor.get_train_examples(data_dir)\n",
    "                elif mode == 'un_train':\n",
    "                    examples = processor.get_un_train_examples(data_dir)\n",
    "                elif mode == 'test':\n",
    "                    examples = processor.get_test_examples(data_dir)\n",
    "                elif mode == 'test_clue':\n",
    "                    examples = CLUE_processors_mapping[(task_name+'-clue').lower()].get_test_examples(data_dir)\n",
    "                output_file = open(os.path.join(output_dir, mode+'.csv'), 'w')\n",
    "                for example in examples:\n",
    "                    text_a = example.text_a\n",
    "                    text_b = example.text_b\n",
    "                    label = example.label\n",
    "                    try:\n",
    "                        output_file.write(text_a+'\\t')\n",
    "                    except:\n",
    "                        continue\n",
    "                        print(example)\n",
    "                    if text_b is not None:\n",
    "                        output_file.write(text_b+'\\t')\n",
    "                    output_file.write(str(label)+'\\n')\n",
    "                output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d7e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055813ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for  task_name in [ 'MNLI']\n",
    "    for k in [10, 20, 30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            output_dir = ust+ '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['train', 'un_train', 'test', 'test-clue']:\n",
    "                if mode == 'train':\n",
    "                    examples = processor.get_train_examples(data_dir)\n",
    "                elif mode == 'un_train':\n",
    "                    examples = processor.get_un_train_examples(data_dir)\n",
    "                output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\n",
    "                for example in examples:\n",
    "                    text_a = example.text_a\n",
    "                    text_b = example.text_b\n",
    "                    label = example.label\n",
    "                    output_file.write(text_a+'\\t')\n",
    "                    if text_b is not None:\n",
    "                        output_file.write(text_b+'\\t')\n",
    "                    output_file.write(label+'\\n')\n",
    "                output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1639f465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for example in train_examples:\n",
    "    print(example.text_b is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "054b49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for  task_name in [ 'QQP', 'RTE']:\n",
    "    for k in [ 30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            \n",
    "            \n",
    "            output_dir = clue + '/'+task_name+'/'+str(100)+'-'+str(seed)\n",
    "            #os.mkdir(output_dir)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "#             try:\n",
    "#                 os.remove(os.path.join(output_dir, 'test-clue.tsv'))\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['train', 'un_train', 'test', 'test_clue']:\n",
    "                lines = []\n",
    "                if mode == 'train':\n",
    "                    examples = []\n",
    "                    for k in [30, 35]:\n",
    "                        if k == 35:\n",
    "                            new_seed = [100, 13, 21, 42, 87][seed-1]\n",
    "                            new_data_dir = '/home/t-yaqingwang/Projects/Few-shot-Learning/data/k-shot' + '/'+task_name+'/'+str(k)+'-'+str(new_seed)\n",
    "                            f = open(os.path.join(new_data_dir, 'train.tsv'))\n",
    "                        else:\n",
    "                            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "                            \n",
    "                            f = open(os.path.join(data_dir, 'train.tsv'))\n",
    "                        if k ==35:\n",
    "                \n",
    "                            lines += f.readlines()[1:]\n",
    "                        else:\n",
    "                            lines += f.readlines()\n",
    "\n",
    "                elif mode == 'un_train':\n",
    "                    \n",
    "                    f = open(os.path.join(data_dir, 'un_train.tsv'))\n",
    "                    lines += f.readlines()\n",
    "                elif mode == 'test':\n",
    "                    f = open(os.path.join(data_dir, 'test.tsv'))\n",
    "                    lines += f.readlines()\n",
    "                elif mode == 'test_clue':\n",
    "                    f = open(os.path.join(data_dir, 'test_clue.tsv'))\n",
    "                    lines += f.readlines()\n",
    "                    \n",
    "                \n",
    "                output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\n",
    "               \n",
    "                for line in lines:\n",
    "                    output_file.write(line)\n",
    "                output_file.close()\n",
    "                  \n",
    "#                     output_file.write(text_a+'\\t')\n",
    "#                     if text_b is not None:\n",
    "#                         output_file.write(text_b+'\\t')\n",
    "#                     output_file.write(label+'\\n')\n",
    "#                 output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d2869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for  task_name in [ 'MNLI']:\n",
    "    for k in [30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            output_dir = clue + '/'+task_name+'/'+str(100)+'-'+str(seed)\n",
    "            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            #os.mkdir(output_dir)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['un_train', 'test_matched', 'test_mismatched', 'test_clue']:\n",
    "                \n",
    "                lines = []\n",
    "\n",
    "                f = open(os.path.join(data_dir, mode+'.tsv'))\n",
    "                lines = f.readlines()\n",
    "\n",
    "                    \n",
    "                output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\n",
    "                for line in lines:\n",
    "                    \n",
    "                    output_file.write(line)\n",
    "                output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lines(lines):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "K_shot = 23\n",
    "\n",
    "for  task_name in [ 'MNLI']:\n",
    "    for k in [ 30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            output_dir = ust+ '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['train']:\n",
    "                lines = []\n",
    "                if mode == 'train':\n",
    "                    examples = []\n",
    "                    \n",
    "                    for k in [30, K_shot]:\n",
    "                        if k == K_shot:\n",
    "                            new_seed = [100, 13, 21, 42, 87][seed-1]\n",
    "                            new_data_dir = '/home/t-yaqingwang/Projects/Few-shot-Learning/data/k-shot' + '/'+task_name+'/'+str(k)+'-'+str(new_seed)\n",
    "                            f = open(os.path.join(new_data_dir, 'train.tsv'))\n",
    "                        else:\n",
    "                            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "                            \n",
    "                            f = open(os.path.join(data_dir, mode+'.tsv'))\n",
    "                        if k == K_shot:\n",
    "                \n",
    "                            lines += f.readlines()[1:]\n",
    "                            import pdb\n",
    "                            pdb.set_trace()\n",
    "                    \n",
    "                        else:\n",
    "                            lines += f.readlines()\n",
    "\n",
    "                else:\n",
    "                    f = open(os.path.join(data_dir, mode+'.tsv'))\n",
    "                    lines = f.readlines()\n",
    "                    \n",
    "                    \n",
    "                output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\n",
    "                for line in lines:\n",
    "                        output_file.write(line)\n",
    "                output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29025ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(subset, original_set, pairwise=True):\n",
    "    guid_dict = {}\n",
    "    \n",
    "    if isinstance(original_set, dict):\n",
    "        \n",
    "        set_dict = original_set\n",
    "        \n",
    "    else:\n",
    "        set_dict = {}\n",
    "    \n",
    "        for i, example in enumerate(original_set):\n",
    "            text_a = example.text_a\n",
    "            guid = example.guid\n",
    "            text_b = example.text_b\n",
    "            if pairwise:\n",
    "                set_dict[text_a.lower()+''+text_b.lower()] = guid\n",
    "            else:\n",
    "                set_dict[text_a.lower()] = guid\n",
    "            \n",
    "    for example in subset:\n",
    "        text_a = example.text_a\n",
    "        text_b = example.text_b\n",
    "        if pairwise:\n",
    "            set_key = text_a.lower() + ''+text_b.lower()\n",
    "        else:\n",
    "            set_key = text_a.lower()\n",
    "        if set_key in set_dict:\n",
    "            guid_dict[set_dict[set_key]] = True\n",
    "        else:\n",
    "            print(set_key)\n",
    "    return guid_dict\n",
    "        \n",
    "\n",
    "            \n",
    "       \n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4458ef8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-87db7fb03a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'un_train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_un_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test_machted'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f5f51d830be3>\u001b[0m in \u001b[0;36mget_un_train_examples\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;34m\"\"\"See base class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LOOKING AT {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"un_train.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_tsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"un_train.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"un_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dev_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bestfriend36/lib/python3.6/site-packages/transformers/data/processors/utils.py\u001b[0m in \u001b[0;36m_read_tsv\u001b[0;34m(cls, input_file, quotechar)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;34m\"\"\"Reads a tab separated value file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8-sig\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquotechar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bestfriend36/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for  task_name in [ 'MNLI']:\n",
    "    for k in [10, 20, 30]:\n",
    "        data_dict[k] = {}\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            data_dict[k][seed] = {}\n",
    "            \n",
    "            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            #output_dir = ust+ '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            #Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            example_list = [] \n",
    "            for mode in ['train', 'un_train']:\n",
    "                if mode == 'train':\n",
    "                    examples = processor.get_train_examples(data_dir)\n",
    "                elif mode == 'un_train':\n",
    "                    examples = processor.get_un_train_examples(data_dir)\n",
    "                elif mode == 'test_machted':\n",
    "                    examples = processor.get_test_examples(data_dir)\n",
    "                elif mode == 'test_mismatched':\n",
    "                    examples = CLUE_processors_mapping['mnli-mm'].get_test_examples(data_dir)\n",
    "                elif mode == 'test_clue':\n",
    "                    examples = CLUE_processors_mapping[(task_name+'-clue').lower()].get_test_examples(data_dir)\n",
    "                example_list.append(examples)\n",
    "                    \n",
    "            data_dict[k][seed] = match(example_list[0], example_list[1])\n",
    "          \n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9644ef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 1\n",
      "30 2\n",
      "30 3\n",
      "30 4\n",
      "30 5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dict = {}\n",
    "keep = False\n",
    "\n",
    "for  task_name in [ 'MNLI']:\n",
    "    for k in [30]:\n",
    "        data_dict[k] = {}\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            print(k, seed)\n",
    "            data_dict[k][seed] = {}\n",
    "            \n",
    "            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            output_dir = clue+ '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "            #Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            example_list = [] \n",
    "            for mode in ['train', 'un_train']:\n",
    "                if mode == 'train':\n",
    "                    examples = processor.get_train_examples(data_dir)\n",
    "                elif mode == 'un_train':\n",
    "                    if not keep:\n",
    "                        examples = processor.get_un_train_examples(data_dir)\n",
    "                        keep = True\n",
    "                        set_dict = {}\n",
    "                        f = open(os.path.join(data_dir, mode+'.tsv'))\n",
    "                        lines = f.readlines()\n",
    "                        for i, example in enumerate(examples):\n",
    "                            try:\n",
    "                                text_a = example.text_a\n",
    "                            except:\n",
    "                                import pdb\n",
    "                                pdb.set_trace()\n",
    "                            guid = example.guid\n",
    "                            text_b = example.text_b\n",
    "                            \n",
    "                            set_dict[text_a.lower()+''+text_b.lower()] = guid\n",
    "                           \n",
    "                elif mode == 'test_machted':\n",
    "                    examples = processor.get_test_examples(data_dir)\n",
    "                elif mode == 'test_mismatched':\n",
    "                    examples = CLUE_processors_mapping['mnli-mm'].get_test_examples(data_dir)\n",
    "                elif mode == 'test_clue':\n",
    "                    examples = CLUE_processors_mapping[(task_name+'-clue').lower()].get_test_examples(data_dir)\n",
    "                example_list.append(examples)\n",
    "                \n",
    "            if keep:\n",
    "                    \n",
    "                data_dict[k][seed] = match(example_list[0], set_dict)\n",
    "            else:\n",
    "                data_dict[k][seed] = match(example_list[0], example_list[1])\n",
    "                \n",
    "                \n",
    "            \n",
    "            train_output_file = open(os.path.join(output_dir, 'train_original_format.tsv'), 'w+')\n",
    "            \n",
    "            selected_list, remain_list = index_data(data_dict[k][seed], lines)\n",
    "            \n",
    "           \n",
    "\n",
    "            for line in selected_list:\n",
    "                train_output_file.write(line)\n",
    "            train_output_file.close()\n",
    "\n",
    "            un_train_output_file = open(os.path.join(output_dir, 'un_train_original.tsv'), 'w+')\n",
    "            for line in remain_list:\n",
    "                un_train_output_file.write(line)\n",
    "            un_train_output_file.close()\n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "          \n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf8360c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_data(idx_dict, lines):\n",
    "    selected_list = [] \n",
    "    remain_list = []\n",
    "    new_idx_dict = {}\n",
    "    for l in idx_dict:\n",
    "        new_idx_dict[l.split('-')[1]] = idx_dict[l]\n",
    "    idx_dict = new_idx_dict\n",
    "    for i, l in enumerate(lines):\n",
    "        if i == 0:\n",
    "            selected_list.append(l)\n",
    "            remain_list.append(l)    \n",
    "            continue\n",
    "        idx = l.split('\\t')[0]\n",
    "        if idx in idx_dict:\n",
    "            selected_list.append(l)\n",
    "        else:\n",
    "            remain_list.append(l)\n",
    "    return selected_list, remain_list\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a08d8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "K_shot= 23\n",
    "for  task_name in [ 'MNLI']:\n",
    "    for k in [ 30]:\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            \n",
    "            \n",
    "            output_dir = clue + '/'+task_name+'/'+str(100)+'-'+str(seed)\n",
    "            #os.mkdir(output_dir)\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "#             try:\n",
    "#                 os.remove(os.path.join(output_dir, 'test-clue.tsv'))\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "            processor = CLUE_processors_mapping[task_name.lower()]\n",
    "            for mode in ['train', 'un_train', 'test_matched', 'test_mismatched', 'test_clue']:\n",
    "                lines = []\n",
    "                if mode == 'train':\n",
    "                    examples = []\n",
    "                    for k in [30, K_shot]:\n",
    "                        if k == K_shot:\n",
    "                            new_seed = [100, 13, 21, 42, 87][seed-1]\n",
    "                            new_data_dir = '/home/t-yaqingwang/Projects/Few-shot-Learning/data/k-shot' + '/'+task_name+'/'+str(k)+'-'+str(new_seed)\n",
    "                            f = open(os.path.join(new_data_dir, 'train.tsv'))\n",
    "                        else:\n",
    "                            data_dir = clue + '/'+task_name+'/'+str(k)+'-'+str(seed)\n",
    "                            \n",
    "                            f = open(os.path.join(data_dir, 'train_original_format.tsv'))\n",
    "                        if k == K_shot:\n",
    "                \n",
    "                            lines += f.readlines()[1:]\n",
    "                        else:\n",
    "                            lines += f.readlines()\n",
    "\n",
    "                elif mode == 'un_train':\n",
    "                    \n",
    "                    f = open(os.path.join(data_dir, 'un_train_original.tsv'))\n",
    "                    lines = f.readlines()\n",
    "                   \n",
    "                else:\n",
    "                    f = open(os.path.join(data_dir, mode+'.tsv'))\n",
    "                    lines = f.readlines()\n",
    "                    \n",
    "                \n",
    "                output_file = open(os.path.join(output_dir, mode+'.tsv'), 'w')\n",
    "               \n",
    "                for line in lines:\n",
    "                    output_file.write(line)\n",
    "                output_file.close()\n",
    "                  \n",
    "#                     output_file.write(text_a+'\\t')\n",
    "#                     if text_b is not None:\n",
    "#                         output_file.write(text_b+'\\t')\n",
    "#                     output_file.write(label+'\\n')\n",
    "#                 output_file.close()\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fdc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
