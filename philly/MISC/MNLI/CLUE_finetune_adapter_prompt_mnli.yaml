description: Fintuning-study

target:
  # which virtual cluster you belong to (msrlabs, etc.). Everyone has access to "msrlabs".
  service: amlk8s
#  name: itphyperdgx2cl1
#  vc: hai3
  name: itplabrr1cl1
  vc: resrchvc
  # physical cluster to use (cam, gcr, rr1, rr2) or Azure clusters (eu1, eu2, etc.)
  # eu1(p100) eu2(p40), rr1 (v100) sc1(v100) sc3(v100)
  #cluster: rr1

storage:
  my_storage:
    storage_account_name: yaqing
    container_name: phillytools


environment:
  registry: docker.io
  image: yaqing/pytorch-few-shot:v0.6
#  registry: phillyregistry.azurecr.io
#  setup:
#    - pip install torch torchvision --user
#    - pip install transformers --user
#    - pip install seqeval --user
#    - pip install flashtool --user

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: ../src

data:
   remote_dir: few_shot_glue
   local_dir: ../data


search:
    job_template:
        name: search_{experiment_name:s}_Task_{TASK}_{MODEL}_seed_{SEED}_{TYPE}_{K}_{BS}_{num_train_epochs}_{update_component}_{adapter_init_std}
        sku: 24G1
        command:
          - python run.py
            --task_name {TASK}
            --data_dir $$PT_DATA_DIR/clue/{TASK}/{K}-{SEED}
            --output_dir $$PT_OUTPUT_DIR/
            --overwrite_output_dir
            --do_train
            --overwrite_cache
            --do_eval
            --do_predict
            --model_name_or_path {MODEL}
            --few_shot_type {TYPE}
            --num_k {K}
            --max_seq_length {max_seq_length}
            --per_device_train_batch_size {BS}
            --per_device_eval_batch_size 16
            --gradient_accumulation_steps {GS}
            --learning_rate {LR}
            --logging_steps {EVAL_STEP}
            --eval_steps {EVAL_STEP}
            --num_train_epochs {num_train_epochs}
            --seed {SEED}
            --contrast_training {CT}
            --template {TEMPLATE}
            --psuedo_selection_opt {OPT}
            --soft_label {SOFT_LABEL}
            --is_semi {SEMI}
            --continuous_prompt {continuous_prompt}
            --un_train_batch_size {un_train_batch_size}
            --self_training_start_iter {self_training_start_iter}
            --sampling_steps {sampling_steps}
            --meta_train_batch_size {MBS}
            --update_teacher_steps {update_teacher_steps}
            --prompt_encoder_type {prompt_encoder_type}
            --prompt_length {prompt_length}
            --demo_condon_steps {demo_condon_steps}
            --prompt_learning_rate {prompt_LR}
            --use_clue
            --use_last_epoch
            --update_component {update_component}
            --adapter_dim {adapter_dim}
            --adapter_choice {adapter_choice}
            --adapter_init_std {adapter_init_std}
    type: grid
    max_trials: 1000
    params:
      - name: TASK
        spec: discrete
        values: ['MNLI']
      - name: MODEL
        spec: discrete
        values: ['roberta-large'] # 'bert-large-uncased', 'bert-base-uncased', 'roberta-base', 'roberta-large', 'microsoft/deberta-base', 'microsoft/deberta-large']
      - name: TYPE
        spec: discrete
        values: ['prompt']
      - name: BS
        spec: discrete
        values: [4]
      - name: MBS
        spec: discrete
        values: [4]
      - name: GS
        spec: discrete
        values: [1]
      - name: max_seq_length
        spec: discrete
        values: [256]
      - name: K
        spec: discrete
        values: [10, 20, 30] #, 20, 30]
      - name: LR
        spec: discrete
        values: [5e-6] #1e-2, 1e-3]
      - name: num_train_epochs
        spec: discrete
        values: [400]
      - name: EVAL_STEP
        spec: discrete
        values: [50]
      - name: SEED
        spec: discrete
        values: [1, 2, 3, 4, 5] #, 2, 3, 4, 5]
      - name: CT
        spec: discrete
        values: [0]
      - name: OPT
        spec: discrete
        values: ['none']
      - name: SOFT_LABEL
        spec: discrete
        values: [0]
      - name: SEMI
        spec: discrete
        values: [0]
      - name: TEMPLATE
        spec: discrete
        values: ['*cls**sent-_0*?*mask*,*+sentl_1**sep+*']
      - name: continuous_prompt
        spec: discrete
        values: [0]
      - name: un_train_batch_size
        spec: discrete
        values: [16]
      - name: self_training_start_iter
        spec: discrete
        values: [10000]
      - name: sampling_steps
        spec: discrete
        values: [1]
      - name: update_teacher_steps
        spec: discrete
        values: [3000]
      - name: prompt_encoder_type
        spec: discrete
        values: ['None']
      - name: prompt_length
        spec: discrete
        values: [80]
      - name: demo_condon_steps
        spec: discrete
        values: [0]
      - name: prompt_LR
        spec: discrete
        values: [1e-3]
      - name: update_component
        spec: discrete
        values: ['adapter'] #[  'feedforward']
      - name: adapter_dim
        spec: discrete
        values: ['128']
      - name: adapter_choice
        spec: discrete
        values: [ 'linear_after' ]
      - name: adapter_init_std
        spec: discrete
        values: [0.0002, 0.002, 0.02, 0.2, 1]





#      - name: MAPPING
#        spec: discrete
#        values: ["'{''contradiction'':''No'',''entailment'':''Yes'',''neutral'':''Maybe'''}"]
#      - name: TASK_EXTRA
#        spec: discrete
#        values: ["--max_seq_len 256 --num_sample 1"]






